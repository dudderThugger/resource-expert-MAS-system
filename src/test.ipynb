{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating open source models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_copilot.models.utils.agents_util import Speaker\n",
    "from agentic_copilot.models.utils.llm_utils import LLMModels\n",
    "from agentic_copilot.models.utils.agent_base import AgentFrameWork\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tests.reliability_testing import arun_single_test\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/scenarios.csv\", mode=\"r\", encoding=\"utf-8-sig\") as f:\n",
    "    scenarios = pd.read_csv(f, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running simple test cases on workers to check whether they are capable of efficent tool handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_source_models = [model.value for model in[LLMModels.MIXTRAL_8X7B, LLMModels.LLAMA_GROQ_3_70B, LLMModels.GEMMA_2_9B]]\n",
    "frameworks = [AgentFrameWork.BASE.value, AgentFrameWork.PROMPT.value]\n",
    "worker_agents = [worker.value for worker in Speaker if worker not in [Speaker.ORCHESTRATOR, Speaker.QUERY_ORCHESTRATOR]]\n",
    "simple_scenarios = scenarios[(scenarios['type'] == 'simple') &\n",
    "                             (scenarios['agent'].isin(worker_agents))]\n",
    "print(\"Models: \", ', '.join(open_source_models))\n",
    "print(\"Frameworks: \", ', '.join(frameworks))\n",
    "print(\"Worker agents: \", ', '.join(worker_agents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating test cases for open-source models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_combinations = [{'model': model, 'agent_framework': framework} for framework in frameworks for model in open_source_models]\n",
    "params_df = pd.DataFrame(parameter_combinations)\n",
    "\n",
    "open_tests_df = params_df.merge(simple_scenarios, how='cross')\n",
    "open_tests_df['response_message'] = open_tests_df['response_message'].apply(lambda x: ('' if pd.isna(x) else x))\n",
    "open_tests_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for _, scenario in simple_scenarios.iterrows():\n",
    "    for framework in frameworks:\n",
    "        test_cases = open_tests_df[(open_tests_df['agent_framework'] == framework) &\n",
    "                                   (open_tests_df['test_case'] == scenario['test_case'])]\n",
    "        tasks = [arun_single_test(id=f\"{params['model']}_{params['agent_framework']}_{params['id']}\", \n",
    "                                  agent_speaker=params['agent'],\n",
    "                                  agent_framework=params['agent_framework'],\n",
    "                                  model=params['model'],\n",
    "                                  question=params['question'],\n",
    "                                  expected_response=(params['response_status'], params['response_message']),\n",
    "                                  input_state_path=params['input_state'],\n",
    "                                  expected_output_state_path=params['expected_state']) for _, params in test_cases.iterrows()]\n",
    "        results.extend(await asyncio.gather(*tasks))\n",
    "        print(\"Results arrived \", len(results), \" / \", len(open_tests_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['framework'] = results_df['framework'].apply(lambda x: ('generic' if x == 'base'  else x))\n",
    "\n",
    "def is_rate(reasoning):\n",
    "    if isinstance(reasoning, dict):\n",
    "        return 'Error code: 429' in reasoning['exception_message']\n",
    "\n",
    "# throwing away rate limit errors\n",
    "all_results = results_df[results_df['reasoning'].apply(lambda x: (not is_rate(x)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = all_results[['model', 'framework', 'result', 'token']].groupby(['model', 'framework']).mean()\n",
    "with open('data/open_source.csv', 'w') as f:\n",
    "    table.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "sns.barplot(data=table, x='model', y='result', hue='framework', ax=axes[0])\n",
    "axes[0].set_title('p@1 by LLM and LLM-agent framework')\n",
    "axes[0].set_ylabel('Result')\n",
    "\n",
    "sns.barplot(data=table, x='model', y='token', hue='framework', ax=axes[1])\n",
    "axes[1].set_title('Total tokens by LLM and LLM-agent framework')\n",
    "axes[1].set_ylabel('Tokens')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('data/open_source.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorizing failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mapping = {\n",
    "    \"Response status different\": \"Requested user input when not needed\",\n",
    "    \"Agent didn't use its return_direct tools\": \"Agent didn't use its formatting tools\",\n",
    "    \"'litellm.BadRequestError: GroqException - {\\\"error\\\":{\\\"message\\\":\\\"Failed to call a function.\": \"Bad tool usage\",\n",
    "    \"str.join() takes exactly one argument\": \"Some error in query\"\n",
    "}\n",
    "\n",
    "def map_category(reasoning):\n",
    "    if isinstance(reasoning, dict):\n",
    "        message = reasoning['exception_message']\n",
    "        for key, value in category_mapping.items():\n",
    "            if key in message:\n",
    "                return value\n",
    "        \n",
    "        return \"other\"\n",
    "    else:\n",
    "        None\n",
    "\n",
    "all_results['category'] = all_results['reasoning'].apply(map_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results['framework'] = all_results['framework'].apply(lambda x: ('generic' if x == 'base' else x))\n",
    "table = all_results[['model', 'framework', 'category', 'id']].groupby(['model', 'framework', 'category']).count()\n",
    "table = table.rename({'id': 'count'})\n",
    "\n",
    "with open('data/categories.csv', 'w') as f:\n",
    "    table.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Enterprise models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enterprise_models = [LLMModels.GPT_4O, LLMModels.GPT_4O_MINI, LLMModels.CLAUDE_3_5_SONNET, LLMModels.CLAUDE_3_5_HAIKU]\n",
    "frameworks = [AgentFrameWork.PROMPT, AgentFrameWork.BASE]\n",
    "parameter_combinations = [{'model': model, 'agent_framework': framework.value} for framework in frameworks for model in enterprise_models]\n",
    "params_df = pd.DataFrame(parameter_combinations)\n",
    "agents = [worker.value for worker in Speaker if worker not in [Speaker.ORCHESTRATOR]]\n",
    "\n",
    "tests = params_df.merge(scenarios, how='cross')\n",
    "tests['response_message'] = tests['response_message'].apply(lambda x: ('' if pd.isna(x) else x))\n",
    "len(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "runs = tests\n",
    "experience = \"e1\"\n",
    "\n",
    "for i in range(0, 5):\n",
    "    tasks = [arun_single_test(id=f\"{params['model'].value}_{params['agent_framework']}_{experience}_{params['id']}_run{i}\",\n",
    "                            test_id=params['id'], \n",
    "                            agent_speaker=params['agent'],\n",
    "                            agent_framework=params['agent_framework'],\n",
    "                            model=params['model'],\n",
    "                            question=params['question'],\n",
    "                            expected_response=(params['response_status'], params['response_message']),\n",
    "                            input_state_path=params['input_state'],\n",
    "                            expected_output_state_path=params['expected_state']) for _, params in runs.iterrows()]\n",
    "    results.extend(await asyncio.gather(*tasks))\n",
    "    print(\"Results arrived \", len(results), \" / \", len(runs) * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/enterprise_results.json', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all = pd.DataFrame(results)\n",
    "results_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all['test_id'] = results_all['id'].apply(lambda x: int(x.split(\"_\")[-2]))\n",
    "results_all.loc[results_all['model'] == 'claude-3-5-haiku', 'price'] *= 3.2\n",
    "results_all_w_type = pd.merge(left=results_all, right=scenarios[['id', 'type']], left_on='test_id', right_on='id', how='left').drop(columns=['id_y', 'test_id'])\n",
    "results_all_w_type.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_grouped = results_all[['agent', 'model', 'framework', 'time']].groupby(['agent', 'model', 'framework']).mean()\n",
    "results_grouped.loc[:, 'claude-3-5-haiku', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_grouped = results_all[['agent', 'model', 'framework', 'result']].groupby(['agent', 'model', 'framework']).mean()\n",
    "results_grouped\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "agents = results_grouped.index.get_level_values('agent').drop_duplicates()\n",
    "agents_name_mapping = {'research_agent': 'Research agent',\n",
    " 'datastream_query_agent': 'DataStream Query agent',\n",
    " 'calculation_agent': 'Calculation agent',\n",
    " 'planning_agent': 'Planning agent',\n",
    " 'invoice_query_agent': 'Invoice agent'}\n",
    "models_mapping = {'gpt-4o': 'GPT-4o',\n",
    " 'gpt-4o-mini': 'GPT-4o mini',\n",
    " 'claude-3-5-haiku': 'Claude 3.5 Haiku',\n",
    " 'claude-3-5-sonnet': 'Claude 3.5 Sonnet'}\n",
    "models = results_grouped.index.get_level_values('model').drop_duplicates()\n",
    "frameworks = results_grouped.index.get_level_values('framework').drop_duplicates()\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=len(agents),\n",
    "    shared_yaxes=True,\n",
    "    subplot_titles=[agents_name_mapping[agent] for agent in agents],\n",
    "    horizontal_spacing=0.01,\n",
    "    \n",
    ")\n",
    "\n",
    "for i, framework in enumerate(frameworks):\n",
    "    for j, agent in enumerate(agents):\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=[models_mapping[model] for model in models],\n",
    "                y=results_grouped.loc[agent, :, framework]['result'],\n",
    "                marker_color='#6A7FDB' if framework == 'prompt' else '#FF6F61',\n",
    "                showlegend= False if j != 1 else True,\n",
    "                name='Optimized prompt' if framework == 'prompt' else 'Generic prompt'\n",
    "            ),\n",
    "            row=1, col=j+1,\n",
    "        )\n",
    "\n",
    "fig.update_layout(title=\"P@1 rate of enterprise models by differnt agents\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_grouped = results_all[['agent', 'model', 'framework', 'price']].groupby(['agent', 'model', 'framework']).mean()\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=len(agents),\n",
    "    shared_yaxes=True,\n",
    "    subplot_titles=[agents_name_mapping[agent] for agent in agents],\n",
    "    horizontal_spacing=0.01,\n",
    "    \n",
    ")\n",
    "\n",
    "for i, framework in enumerate(frameworks):\n",
    "    for j, agent in enumerate(agents):\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=[models_mapping[model] for model in models],\n",
    "                y=results_grouped.loc[agent, :, framework]['price'],\n",
    "                marker_color='#6A7FDB' if framework == 'prompt' else '#FF6F61',\n",
    "                showlegend= False if j != 1 else True,\n",
    "                name='Optimized prompt' if framework == 'prompt' else 'Generic prompt'\n",
    "            ),\n",
    "            row=1, col=j+1,\n",
    "        )\n",
    "\n",
    "fig.update_layout(title=\"Mean of the calls' prices by enterprise models and agents\")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_grouped = results_all[['agent', 'model', 'framework', 'token']].groupby(['agent', 'model', 'framework']).mean()\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=len(agents),\n",
    "    shared_yaxes=True,\n",
    "    subplot_titles=[agents_name_mapping[agent] for agent in agents],\n",
    "    horizontal_spacing=0.01,\n",
    "    \n",
    ")\n",
    "\n",
    "for i, framework in enumerate(frameworks):\n",
    "    for j, agent in enumerate(agents):\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=[models_mapping[model] for model in models],\n",
    "                y=results_grouped.loc[agent, :, framework]['token'],\n",
    "                marker_color='#6A7FDB' if framework == 'prompt' else '#FF6F61',\n",
    "                showlegend= False if j != 1 else True,\n",
    "                name='Optimized prompt' if framework == 'prompt' else 'Generic prompt'\n",
    "            ),\n",
    "            row=1, col=j+1,\n",
    "        )\n",
    "\n",
    "fig.update_layout(title=\"Total tokens by enterprise models and agents\")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = results_all[['result', 'time', 'token', 'price']].select_dtypes(include=['float64', 'int64']).corr()\n",
    "px.imshow(c, text_auto=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model for model in results_all[['model']].drop_duplicates()['model']]\n",
    "\n",
    "illustrate = results_all_w_type[(~results_all_w_type['agent'].isin(['planning_agent'])) &\n",
    "                          (results_all_w_type['time'] < 50)].sort_values(by='time')[['time', 'model', 'type']]\n",
    "\n",
    "bins = 30\n",
    "fixed_bins = np.linspace(0, illustrate['time'].max() + 1, bins)\n",
    "bin_centers = (fixed_bins[1:] + fixed_bins[:-1]) / 2\n",
    "colors = [\"#3B4CCA\", \"#77DD77\", \"#FFD700\", \"#FF5733\"]\n",
    "width = (fixed_bins[-1] - fixed_bins[0]) / bins\n",
    "\n",
    "fig, axs = plt.subplots(nrows=4, ncols=2, sharex=True, sharey=True, figsize=(12,8))\n",
    "\n",
    "for i, type in enumerate(['simple', 'complex']):\n",
    "    for j, model in enumerate(models):\n",
    "        counts, bins = np.histogram(illustrate[(illustrate['model'] == model) &\n",
    "                                               (illustrate['type'] == type)]['time'], bins=fixed_bins)\n",
    "        axs[j][i].bar(bin_centers, counts, width=width, color=colors[i])\n",
    "        axs[j][i].set_title(f\"{models_mapping[model]}\")\n",
    "\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "fig.text(0.5, 0.04, \"Response time\", ha='center', fontsize=14)\n",
    "fig.text(0.27, 0.065, \"Simple test scenarios\", ha='center', fontsize=10)\n",
    "fig.text(0.73, 0.065, \"Complex test scenarios\", ha='center', fontsize=10)\n",
    "fig.savefig('data/response_times.png')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-copilot-XQyoHElM-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
